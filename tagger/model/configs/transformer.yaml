
model: TransformerModel
run_config :
  verbose : 2
  debug : True
model_config :
  name : transformer
  emb_layers : [8,8]
  transformer_layers : [[2,4,2,8],[2,4,2,8]] #num_heads, mha_hidden_dim, num_dense_layers, dim_dense_layers
  classification_layers : [16,10]
  regression_layers : [8]
  kernel_initializer : 'lecun_uniform'
quantization_config:
training_config :
  weight_method: "onlyclass"
  epochs : 100
  batch_size : 1024
  learning_rate : 0.001
  validation_split : 0.1
  loss_weights : [1,1]
  initial_sparsity : 0.0
  final_sparsity : 0.1
  EarlyStopping_patience : 10
  ReduceLROnPlateau_factor : 0.5
  ReduceLROnPlateau_patience : 5
  ReduceLROnPlateau_min_lr : 0.00001
hls4ml_config: